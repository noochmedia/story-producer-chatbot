import os
import requests
from utils.logger import logger
from services.prompt_manager import PromptManager
from services.conversation_manager import ConversationManager
from services.transcript_service import TranscriptService
from prompts.base_system import BASE_SYSTEM_PROMPT

class ChatService:
    def __init__(self):
        self.api_url = os.getenv('MISTRAL_API_URL')
        self.api_key = os.getenv('MISTRAL_API_KEY')
        if not self.api_url:
            raise ValueError("Missing required environment variable: MISTRAL_API_URL")
        
        self.prompt_manager = PromptManager()
        self.conversation_manager = ConversationManager()
        self.transcript_service = TranscriptService()
        self.base_prompt = self._get_system_prompt()

    def _split_content(self, text, max_tokens=24000):
        """Split content into chunks that fit within token limits"""
        # Rough estimate of tokens (characters / 4 is a conservative estimate)
        chars_per_token = 4
        max_chars = max_tokens * chars_per_token
        
        # Split content into chunks
        chunks = []
        current_chunk = ""
        current_chars = 0
        
        paragraphs = text.split('\n\n')
        for paragraph in paragraphs:
            paragraph_chars = len(paragraph)
            
            if current_chars + paragraph_chars > max_chars:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = paragraph
                current_chars = paragraph_chars
            else:
                if current_chunk:
                    current_chunk += '\n\n'
                current_chunk += paragraph
                current_chars += paragraph_chars + 2  # +2 for newlines
                
        if current_chunk:
            chunks.append(current_chunk)
            
        return chunks

    def _get_system_prompt(self, chunk_index=0):
        """Get system prompt with chunked transcript content"""
        try:
            transcript_files = self.transcript_service.get_transcript_list()
            
            if not transcript_files:
                return BASE_SYSTEM_PROMPT.format(
                    available_transcripts="No transcripts currently loaded."
                )
            
            # Format transcript list with basic info and collect full content
            transcript_list = []
            full_content = ""
            
            for transcript_id in transcript_files:
                try:
                    path = os.path.join('transcripts', transcript_id)
                    with open(path, 'r') as f:
                        content = f.read()
                        word_count = len(content.split())
                        transcript_list.append(f"- {transcript_id} ({word_count} words)")
                        full_content += f"\n\n### {transcript_id} ###\n{content}"
                except Exception as e:
                    logger.error(f"Error reading transcript {transcript_id}: {str(e)}")
                    continue
            
            # Format the available transcripts section with chunk info
            transcripts_info = "\n".join(transcript_list)
            
            if not transcripts_info:
                transcripts_info = "Error: Unable to read transcript contents."
            
            # Split content into chunks
            content_chunks = self._split_content(full_content)
            total_chunks = len(content_chunks)
            
            # Get the requested chunk (or first chunk if index is out of range)
            chunk_index = min(chunk_index, total_chunks - 1)
            current_chunk = content_chunks[chunk_index]
            
            # Create system prompt with chunk information
            system_prompt = BASE_SYSTEM_PROMPT.format(
                available_transcripts=f"{transcripts_info}\n\nViewing content chunk {chunk_index + 1} of {total_chunks}"
            )
            
            return system_prompt + "\n\nTRANSCRIPT CONTENTS (Part {} of {}):\n".format(
                chunk_index + 1, total_chunks
            ) + current_chunk
            
        except Exception as e:
            logger.error(f"Error preparing system prompt: {str(e)}")
            return BASE_SYSTEM_PROMPT.format(
                available_transcripts="Error loading transcript information"
            )

    def get_chat_response(self, prompt, session_id='default', chunk_index=0):
        """Get a response from our self-hosted model using chunked content"""
        try:
            context = self.conversation_manager.get_context(session_id) or {}
            current_chunk = context.get('current_chunk', 0)
            
            # Use provided chunk_index or continue from last used chunk
            chunk_to_use = chunk_index if chunk_index != 0 else current_chunk
            
            # Start with system message containing chunked content
            messages = [{
                'role': 'system',
                'content': self._get_system_prompt(chunk_to_use)
            }]
            
            # Add conversation history
            conversation_history = self.conversation_manager.get_formatted_messages(session_id)
            if conversation_history:
                messages.extend(conversation_history)
                
            # Update the current chunk in context
            self.conversation_manager.update_context(session_id, {'current_chunk': chunk_to_use})
            
            # Add the new user message
            messages.append({
                'role': 'user',
                'content': prompt
            })
            
            # Log the request details
            request_url = f"{self.api_url}/v1/chat/completions"
            logger.info(f"Making request to: {request_url}")
            logger.info(f"Using chunk {chunk_to_use + 1}")
            
            # Prepare headers and data
            headers = {
                'Content-Type': 'application/json'
            }
            if self.api_key:
                headers['Authorization'] = f'Bearer {self.api_key}'
            
            data = {
                'model': 'mistral-7b-instruct',
                'messages': messages,
                'temperature': 0.7,
                'max_tokens': 2000  # Reduced from 4096 to stay within limits
            }
            
            try:
                # Make API call to our self-hosted model
                response = requests.post(
                    request_url,
                    headers=headers,
                    json=data,
                    verify=False,
                    timeout=30
                )
                
                # Log the response status
                logger.info(f"Response status: {response.status_code}")
                
                if response.status_code == 200:
                    result = response.json()
                    response_content = result['choices'][0]['message']['content']
                    
                    # Store messages in conversation history
                    self.conversation_manager.add_message(session_id, 'user', prompt)
                    self.conversation_manager.add_message(session_id, 'assistant', response_content)
                    
                    # Check if the response indicates need for more context
                    if "I don't see that information in the current section" in response_content.lower():
                        # Try next chunk if available
                        next_chunk = chunk_to_use + 1
                        content_chunks = self._split_content(self._get_system_prompt())
                        if next_chunk < len(content_chunks):
                            logger.info(f"Information not found in current chunk, trying chunk {next_chunk + 1}")
                            return self.get_chat_response(prompt, session_id, next_chunk)
                    
                    return {
                        'response': response_content,
                        'success': True,
                        'chunk_index': chunk_to_use
                    }
                else:
                    if response.status_code == 422:  # Validation error
                        logger.warning("Token limit exceeded, trying with smaller chunk...")
                        return self.get_chat_response(prompt, session_id, chunk_to_use)
                    
                    error_msg = f"API request failed with status {response.status_code}: {response.text}"
                    raise Exception(error_msg)
                    
            except requests.exceptions.ConnectionError:
                error_msg = f"Failed to connect to model at {self.api_url}. Please verify the server is running."
                logger.error(error_msg)
                raise Exception(error_msg)
            except requests.exceptions.Timeout:
                error_msg = "Request to model timed out. The server might be overloaded."
                logger.error(error_msg)
                raise Exception(error_msg)
            except requests.exceptions.RequestException as e:
                error_msg = f"Error communicating with model: {str(e)}"
                logger.error(error_msg)
                raise Exception(error_msg)
                
        except Exception as e:
            logger.error(f"Error in chat response: {str(e)}")
            raise