import os
import requests
from utils.logger import logger
from services.prompt_manager import PromptManager
from services.conversation_manager import ConversationManager
from services.transcript_service import TranscriptService
from services.semantic_index import SemanticIndex
from prompts.base_system import BASE_SYSTEM_PROMPT

class ChatService:
    def __init__(self):
        self.api_url = os.getenv('MISTRAL_API_URL')
        self.api_key = os.getenv('MISTRAL_API_KEY')
        if not self.api_url:
            raise ValueError("Missing required environment variable: MISTRAL_API_URL")
        
        self.prompt_manager = PromptManager()
        self.conversation_manager = ConversationManager()
        self.transcript_service = TranscriptService()
        self.semantic_index = SemanticIndex()
        
        # Initialize semantic index with transcripts
        self.semantic_index.process_transcripts(self.transcript_service)

    def _get_system_prompt(self, relevant_content=None):
        """Get system prompt with relevant transcript content"""
        try:
            transcript_files = self.transcript_service.get_transcript_list()
            
            if not transcript_files:
                return BASE_SYSTEM_PROMPT.format(
                    available_transcripts="No transcripts currently loaded."
                )
            
            # Format transcript list
            transcript_list = []
            for transcript_id in transcript_files:
                try:
                    path = os.path.join('transcripts', transcript_id)
                    with open(path, 'r') as f:
                        content = f.read()
                        word_count = len(content.split())
                        transcript_list.append(f"- {transcript_id} ({word_count} words)")
                except Exception as e:
                    logger.error(f"Error reading transcript {transcript_id}: {str(e)}")
                    continue
            
            # Create base prompt with transcript list
            system_prompt = BASE_SYSTEM_PROMPT.format(
                available_transcripts="\n".join(transcript_list)
            )
            
            # Add relevant content if provided
            if relevant_content:
                content_text = "\n\nRELEVANT TRANSCRIPT SECTIONS:\n"
                for item in relevant_content:
                    content_text += f"\n### From {item['transcript_id']} ###\n{item['text']}\n"
                system_prompt += content_text
            
            return system_prompt
            
        except Exception as e:
            logger.error(f"Error preparing system prompt: {str(e)}")
            return BASE_SYSTEM_PROMPT.format(
                available_transcripts="Error loading transcript information"
            )

    def get_chat_response(self, prompt, session_id='default'):
        """Get a response from our self-hosted model using semantic search"""
        try:
            # Find relevant content for the query
            relevant_content = self.semantic_index.find_relevant_content(prompt, num_results=5)
            
            # Get conversation history
            conversation_history = self.conversation_manager.get_formatted_messages(session_id)
            
            # Create system message with relevant content
            messages = [{
                'role': 'system',
                'content': self._get_system_prompt(relevant_content)
            }]
            
            # Add conversation history
            if conversation_history:
                messages.extend(conversation_history)
            
            # Add the new user message
            messages.append({
                'role': 'user',
                'content': prompt
            })
            
            # Log the request details
            request_url = f"{self.api_url}/v1/chat/completions"
            logger.info(f"Making request to: {request_url}")
            
            # Prepare headers and data
            headers = {
                'Content-Type': 'application/json'
            }
            if self.api_key:
                headers['Authorization'] = f'Bearer {self.api_key}'
            
            data = {
                'messages': messages,
                'temperature': 0.7
            }
            
            try:
                # Make API call to our self-hosted model
                response = requests.post(
                    request_url,
                    headers=headers,
                    json=data,
                    verify=False,
                    timeout=60
                )
                
                # Log the response status
                logger.info(f"Response status: {response.status_code}")
                
                if response.status_code == 200:
                    result = response.json()
                    response_content = result['choices'][0]['message']['content']
                    
                    # Store messages in conversation history
                    self.conversation_manager.add_message(session_id, 'user', prompt)
                    self.conversation_manager.add_message(session_id, 'assistant', response_content)
                    
                    return {
                        'response': response_content,
                        'success': True
                    }
                else:
                    error_msg = f"API request failed with status {response.status_code}: {response.text}"
                    raise Exception(error_msg)
                    
            except requests.exceptions.ConnectionError:
                error_msg = f"Failed to connect to model at {self.api_url}. Please verify the server is running."
                logger.error(error_msg)
                raise Exception(error_msg)
            except requests.exceptions.Timeout:
                error_msg = "Request to model timed out. The server might be overloaded."
                logger.error(error_msg)
                raise Exception(error_msg)
            except requests.exceptions.RequestException as e:
                error_msg = f"Error communicating with model: {str(e)}"
                logger.error(error_msg)
                raise Exception(error_msg)
                
        except Exception as e:
            logger.error(f"Error in chat response: {str(e)}")
            raise