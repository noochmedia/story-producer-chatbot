import os
import requests
from utils.logger import logger
from services.prompt_manager import PromptManager
from services.conversation_manager import ConversationManager
from services.transcript_service import TranscriptService
from prompts.base_system import BASE_SYSTEM_PROMPT

class ChatService:
    def __init__(self):
        self.api_url = os.getenv('MISTRAL_API_URL')
        self.api_key = os.getenv('MISTRAL_API_KEY')
        if not self.api_url:
            raise ValueError("Missing required environment variable: MISTRAL_API_URL")
        
        self.prompt_manager = PromptManager()
        self.conversation_manager = ConversationManager()
        self.transcript_service = TranscriptService()
        self.base_prompt = self._get_system_prompt()

    def _split_content(self, text, max_tokens=24000):
        """Split content into chunks that fit within token limits"""
        # Rough estimate of tokens (characters / 4 is a conservative estimate)
        chars_per_token = 4
        max_chars = max_tokens * chars_per_token
        
        # Split content into chunks
        chunks = []
        current_chunk = ""
        current_chars = 0
        
        paragraphs = text.split('\n\n')
        for paragraph in paragraphs:
            paragraph_chars = len(paragraph)
            
            if current_chars + paragraph_chars > max_chars:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = paragraph
                current_chars = paragraph_chars
            else:
                if current_chunk:
                    current_chunk += '\n\n'
                current_chunk += paragraph
                current_chars += paragraph_chars + 2  # +2 for newlines
                
        if current_chunk:
            chunks.append(current_chunk)
            
        return chunks

    def _get_system_prompt(self, chunk_index=0):
        """Get system prompt with chunked transcript content"""
        try:
            transcript_files = self.transcript_service.get_transcript_list()
            
            if not transcript_files:
                return BASE_SYSTEM_PROMPT.format(
                    available_transcripts="No transcripts currently loaded."
                )
            
            # Format transcript list with basic info and collect full content
            transcript_list = []
            full_content = ""
            
            for transcript_id in transcript_files:
                try:
                    path = os.path.join('transcripts', transcript_id)
                    with open(path, 'r') as f:
                        content = f.read()
                        word_count = len(content.split())
                        transcript_list.append(f"- {transcript_id} ({word_count} words)")
                        full_content += f"\n\n### {transcript_id} ###\n{content}"
                except Exception as e:
                    logger.error(f"Error reading transcript {transcript_id}: {str(e)}")
                    continue
            
            # Format the available transcripts section with chunk info
            transcripts_info = "\n".join(transcript_list)
            
            if not transcripts_info:
                transcripts_info = "Error: Unable to read transcript contents."
            
            # Split content into chunks
            content_chunks = self._split_content(full_content)
            total_chunks = len(content_chunks)
            
            # Get the requested chunk (or first chunk if index is out of range)
            chunk_index = min(chunk_index, total_chunks - 1)
            current_chunk = content_chunks[chunk_index]
            
            # Create system prompt with chunk information
            system_prompt = BASE_SYSTEM_PROMPT.format(
                available_transcripts=f"{transcripts_info}\n\nViewing content chunk {chunk_index + 1} of {total_chunks}"
            )
            
            return system_prompt + "\n\nTRANSCRIPT CONTENTS (Part {} of {}):\n".format(
                chunk_index + 1, total_chunks
            ) + current_chunk
            
        except Exception as e:
            logger.error(f"Error preparing system prompt: {str(e)}")
            return BASE_SYSTEM_PROMPT.format(
                available_transcripts="Error loading transcript information"
            )

    def get_chat_response(self, prompt, session_id='default'):
        """Get a response from our self-hosted Mistral model"""
        try:
            # Get conversation history
            conversation_history = self.conversation_manager.get_formatted_messages(session_id)
            
            # Start with system message containing all transcript content
            messages = [{
                'role': 'system',
                'content': self._get_system_prompt()
            }]
            
            # Add conversation history
            if conversation_history:
                messages.extend(conversation_history)
            
            # Add the new user message
            messages.append({
                'role': 'user',
                'content': prompt
            })
            
            # Prepare the API call to our self-hosted model
            headers = {
                'Content-Type': 'application/json'
            }
            
            # Add API key if it exists (optional for self-hosted)
            if self.api_key:
                headers['Authorization'] = f'Bearer {self.api_key}'
            
            data = {
                'model': 'mistral-7b-instruct',  # The model name for our self-hosted instance
                'messages': messages,
                'temperature': 0.7,
                'max_tokens': 4096  # Default max tokens for response
            }
            
            # Log the request details
            request_url = f"{self.api_url}/v1/chat/completions"
            logger.info(f"Making request to: {request_url}")
            logger.info(f"Headers: {headers}")
            logger.info(f"Request data: {data}")
            
            try:
                # Make API call to our self-hosted model
                response = requests.post(
                    request_url,
                    headers=headers,
                    json=data,
                    verify=False,  # Since we're using self-hosted
                    timeout=30  # 30-second timeout
                )
                
                # Log the response
                logger.info(f"Response status: {response.status_code}")
                if response.status_code != 200:
                    logger.error(f"Response content: {response.text}")
                    
                    # Check for specific error cases
                    if response.status_code == 404:
                        raise Exception("API endpoint not found. Please verify the model endpoint URL.")
                    elif response.status_code == 401:
                        raise Exception("Authentication failed. Please check API key configuration.")
                    elif response.status_code == 500:
                        raise Exception("Internal server error in the model. Please check model logs.")
                    else:
                        error_msg = f"API request failed with status {response.status_code}: {response.text}"
                        raise Exception(error_msg)
                
                # Parse successful response
                result = response.json()
                response_content = result['choices'][0]['message']['content']
                
                # Store messages in conversation history
                self.conversation_manager.add_message(session_id, 'user', prompt)
                self.conversation_manager.add_message(session_id, 'assistant', response_content)
                
                return {
                    'response': response_content,
                    'success': True
                }
                
            except requests.exceptions.ConnectionError:
                error_msg = f"Failed to connect to model at {self.api_url}. Please verify the server is running."
                logger.error(error_msg)
                raise Exception(error_msg)
            except requests.exceptions.Timeout:
                error_msg = "Request to model timed out. The server might be overloaded."
                logger.error(error_msg)
                raise Exception(error_msg)
            except requests.exceptions.RequestException as e:
                error_msg = f"Error communicating with model: {str(e)}"
                logger.error(error_msg)
                raise Exception(error_msg)

        except Exception as e:
            logger.error(f"Error getting chat response: {str(e)}")
            raise

